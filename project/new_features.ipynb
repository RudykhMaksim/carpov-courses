{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc34f07-23fe-4f16-bd91-8c3dae98407c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from traceback import print_exc #для отладки\n",
    "import os  # Модуль для работы с операционной системой: чтение переменных окружения, управление файлами и путями и т.д.\n",
    "import pickle  # Модуль для сериализации/десериализации Python-объектов в бинарный формат — часто используется для сохранения моделей.\n",
    "\n",
    "import pandas as pd #это pandas, нужен нам для выгрузки таблиц из БД\n",
    "import numpy as np #это numpy, нужен нам для обработки признаков в таблицах\n",
    "from typing import List # Модуль для аннотации типов — в данном случае List используется, чтобы явно указать, что функция возвращает список объектов.\n",
    "from fastapi import FastAPI, HTTPException #это сам FastAPI, с помощью которого мы создаем приложения, а второй модуль - это для отладки\n",
    "from pydantic import BaseModel #это класс, на основании которого мы создаем класс PostGet\n",
    "from datetime import datetime #модуль дает возможность использовать специальный формат данных datetime\n",
    "from sqlalchemy import create_engine #создание движка для обращения к sql\n",
    "from sklearn.preprocessing import LabelEncoder #модуль используется для обработки категориальных фичей посредством замены каждого уникального текстового значения на порядковый номер\n",
    "\n",
    "# ==== Модель ответа ====\n",
    "class PostGet(BaseModel): #создаем класс, который задаёт формат вывода ответа на запрос\n",
    "    id: int #id будет числом\n",
    "    text: str #текст будет строкой\n",
    "    topic: str #топик будет строкой\n",
    "\n",
    "    class Config:\n",
    "        orm_mode = True #этот параметр позволяет FastAPI получать на вход не только словари, но и объекты SQLAlchemy\n",
    "\n",
    "app = FastAPI() #создаем приложение\n",
    "\n",
    "# ==== Подключение к БД ====\n",
    "def batch_load_sql(query: str) -> pd.DataFrame: \n",
    "    #создаем функцию для загрузки данных из SQL. \n",
    "    #query: str - это входной аргумент (SQL-запрос) в формате string\n",
    "    #-> pd.DataFrame - аннотация, что выводом функции будет pandas dataframe\n",
    "    CHUNKSIZE = 200_000 #это размер куска в 200000 строк, который мы вынимаем за раз из БД\n",
    "    engine = create_engine( #создаем движок, говорим ему имя пользователя, пароль, хост, порт, базу\n",
    "        \"postgresql://robot-startml-ro:pheiph0hahj1Vaif@\"\n",
    "        \"postgres.lab.karpov.courses:6432/startml\"\n",
    "    )\n",
    "    conn = engine.connect().execution_options(stream_results=True) \n",
    "    #здесь мы открываем соединение\n",
    "    #stream_results=True - говорит не загружать сразу всю таблицу в память, отдавать рез-ты по мере чтения\n",
    "    chunks = [] #создаем пустой список chunks\n",
    "    for chunk_dataframe in pd.read_sql(query, conn, chunksize=CHUNKSIZE): #начинаем цикл, где для каждого куска dataframe размером CHUNKSIZE...\n",
    "        chunks.append(chunk_dataframe) #мы добавляем этот кусок в список chunks\n",
    "    conn.close() #закрываем соединение\n",
    "    return pd.concat(chunks, ignore_index=True) #в качестве вывода мы возвращаем chunks, превращенный из списка в pandas dataframe\n",
    "\n",
    "# ==== Загрузка модели ====\n",
    "def get_model_path(path: str) -> str: \n",
    "    #создаем функцию для получения пути к модели\n",
    "    #входной параметр path ожидается в формате string, вывод - тоже string\n",
    "    if os.environ.get(\"IS_LMS\") == \"1\": #проверяем значение переменной окружения, чтобы понять, откуда запускается код\n",
    "        return \"/workdir/user_input/model\" #если мы в LMS, функция вернет этот код\n",
    "    return path #во всех остальных случаях вернем то же, что и приняли на входе\n",
    "\n",
    "model_path = get_model_path(\"model.pkl\") #запускаем функцию get_model_path со значением path = \"model.pkl\", результат кладем в model_path\n",
    "with open(model_path, \"rb\") as f: # открываем файл в бинарном режиме\n",
    "    model = pickle.load(f) # загружаем модель из файла\n",
    "\n",
    "# ==== Загрузка признаков пользователей ====\n",
    "user_features = batch_load_sql(\"SELECT * FROM maxezdu_features_lesson_22\")\n",
    "#запускаю функцию batch_load_sql с запросом всей таблицы maxezdu_features_lesson_22, сохраняю её в user_features\n",
    "\n",
    "# ==== Загрузка постов ====\n",
    "posts_df = batch_load_sql(\"SELECT post_id, text, topic FROM post_text_df\")\n",
    "#запускаю функцию batch_load_sql с запросом 3 столбцов таблицы post_text_df, сохраняю её в posts_df\n",
    "\n",
    "# Добавим длину текста как признак поста\n",
    "posts_df['text_len'] = posts_df['text'].str.len()\n",
    "# к датафрейму с постами мы добавляем колонку text_len, \n",
    "# которая получается как длина string из колонки text (.str.len() — это строковой метод pandas)\n",
    "\n",
    "# Загружаем небольшой сэмпл feed_data\n",
    "feed_sample = batch_load_sql(\"SELECT * FROM feed_data LIMIT 200000\")\n",
    "\n",
    "# Оставляем только просмотры\n",
    "feed_sample = feed_sample[feed_sample[\"action\"] == \"view\"]\n",
    "\n",
    "# Считаем статистики постов\n",
    "post_stats = feed_sample.groupby(\"post_id\").agg(\n",
    "    views_y=(\"action\", \"count\"),\n",
    "    likes_y=(\"target\", \"sum\")\n",
    ").reset_index()\n",
    "\n",
    "post_stats[\"ctr_y\"] = post_stats[\"likes_y\"] / post_stats[\"views_y\"]\n",
    "post_stats = post_stats.fillna(0)\n",
    "\n",
    "posts_df = posts_df.merge(post_stats, on=\"post_id\", how=\"left\")\n",
    "posts_df = posts_df.fillna(0)\n",
    "\n",
    "\n",
    "\n",
    "# ==== Категориальные признаки ====\n",
    "cat_cols = ['country', 'city', 'os', 'source', 'topic'] #создаем список, в котором перечисляем категориальные признаки\n",
    "label_encoders = {} #создаем пустой словарь, в который будем класть по ключу 'имя из cat_cols' экземпляр эгкодера с соответствием\n",
    "#уникальных значений в колонке их порядковому номеру\n",
    "\n",
    "# Обучаем LabelEncoder отдельно для пользователей и постов\n",
    "for col in cat_cols: #начинаем цикл по списку cat_cols\n",
    "    le = LabelEncoder() #создаем экземпляр энкодера\n",
    "    if col in user_features.columns: #условие: если col есть среди колонок таблицы user_features, то:\n",
    "        user_features[col] = user_features[col].astype(str) #сделать тип всех значений в этой колонке в user_features стрингом\n",
    "        le.fit(user_features[col]) #применяем fit для le на колонке из user_features. \n",
    "        #Этот метод в итоге сохраняет в переменную le список из преобразовынных в порядковые номера значений колонки user_features[col]?\n",
    "    else: #когда col нет в user_features, берем колонку с именем col в таблице posts_df и делаем такое же преобразование\n",
    "        posts_df[col] = posts_df[col].astype(str)\n",
    "        le.fit(posts_df[col])\n",
    "    label_encoders[col] = le #добавляем в словарь по ключу 'имя из cat_cols' энкодер\n",
    "\n",
    "# ==== Эндпоинт ====\n",
    "@app.get(\"/post/recommendations/\", response_model=List[PostGet]) #создаем метод эндпоинта .get с параметрами: путь и форма ответа\n",
    "def recommended_posts(id: int, time: datetime, limit: int = 5) -> List[PostGet]: \n",
    "    #функция, на вход получает id, время и лимит. Вывод ожидается по форме List\n",
    "    try: #проверка на ошибочку\n",
    "        if id not in user_features[\"user_id\"].values: #если такого user id нет в таблице, пишет User not found\n",
    "            raise HTTPException(status_code=404, detail=\"User not found\")\n",
    "\n",
    "        user_row = user_features[user_features[\"user_id\"] == id].drop(columns=[\"user_id\"]) \n",
    "        #ищем строку в user_features с нужным id, удаляем из этой строки сам user_id, и сохраняем в user_row\n",
    "        #то есть получаем список значений user_features для запрашиваемого юзера\n",
    "\n",
    "        user_row = user_row.rename(columns={'views': 'views_x','likes': 'likes_x','ctr': 'ctr_x'})\n",
    "\n",
    "        # Дублируем user_row на количество постов\n",
    "        user_matrix = pd.concat([user_row] * len(posts_df), ignore_index=True)\n",
    "        #создаем user_matrix, которая есть просто таблица с повторяющейся len(posts_df) раз строкой user_row\n",
    "\n",
    "\n",
    "        # Добавляем постовые признаки\n",
    "        post_feats = posts_df[['topic', 'text_len', 'views_y', 'likes_y', 'ctr_y']].copy()\n",
    "        #создаем копию таблицы постов, в которой только два столбца: топик и длина текста\n",
    "\n",
    "        # Объединяем пользовательские и постовые признаки\n",
    "        data_for_pred = pd.concat([user_matrix.reset_index(drop=True), post_feats.reset_index(drop=True)], axis=1)\n",
    "        # соединяем таблицу из одинаковых строк с признаками юзера с таблицей с признаками topic и text_len из поста\n",
    "\n",
    "        # Кодируем категориальные признаки\n",
    "        for col in cat_cols:\n",
    "            data_for_pred[col] = label_encoders[col].transform(data_for_pred[col].astype(str))\n",
    "            #проходимся по категориальным колонкам в новой таблице data_for_pred и вставляем с \n",
    "            #помощью метода transform значения из сформированных ранее энкодеров\n",
    "\n",
    "\n",
    "        # Предсказания вероятностей\n",
    "        preds = model.predict_proba(data_for_pred)[:, 1]\n",
    "        #выводим в переменную preds вероятности принадлежности каждой пары [запрашиваемый юзер, пост] к положительному классу\n",
    "\n",
    "        # Сортируем посты по предсказанному рейтингу\n",
    "        posts_df_copy = posts_df.copy() #создаем копию таблицы постов\n",
    "        posts_df_copy[\"score\"] = preds #добавляем в эту копию колонку score, заполненную значениями preds\n",
    "        top_posts = posts_df_copy.sort_values(\"score\", ascending=False).head(limit) \n",
    "        #в top_posts сохраняем limit постов из отсортированной по убыванию значения в колонке score таблицы posts_df_copy\n",
    "\n",
    "        return [\n",
    "            PostGet(id=row.post_id, text=row.text, topic=row.topic) #возвращаем ответ по форме PostGet\n",
    "            for _, row in top_posts.iterrows() #значения из строк таблицы top_posts, начиная с верхней\n",
    "        ]\n",
    "\n",
    "    except Exception:\n",
    "        print(\"Ошибка в recommended_posts():\")\n",
    "        print_exc()\n",
    "        raise HTTPException(status_code=500, detail=\"Internal server error\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
